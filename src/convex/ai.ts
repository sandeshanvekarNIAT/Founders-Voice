"use node";

import { v } from "convex/values";
import { action, internalAction } from "./_generated/server";
import { internal } from "./_generated/api";
import { GoogleGenerativeAI } from "@google/generative-ai";
import Groq from "groq-sdk";

// Lazy initialization for Google Gemini (for Report Cards & Mentorship)
function getGeminiClient() {
  const apiKey = process.env.GOOGLE_GEMINI_API_KEY;
  if (!apiKey) {
    throw new Error(
      "GOOGLE_GEMINI_API_KEY is not set. Please set it using: npx convex env set GOOGLE_GEMINI_API_KEY <your-key>\nGet your free key at: https://ai.google.dev/"
    );
  }
  return new GoogleGenerativeAI(apiKey);
}

// Lazy initialization for Groq (for fast VC interruptions)
function getGroqClient() {
  const apiKey = process.env.GROQ_API_KEY;
  if (!apiKey) {
    throw new Error(
      "GROQ_API_KEY is not set. Please set it using: npx convex env set GROQ_API_KEY <your-key>\nGet your free key at: https://console.groq.com/"
    );
  }
  return new Groq({ apiKey });
}

// Generate the Fundability Report Card using Google Gemini 1.5 Flash
export const generateReportCard = internalAction({
  args: { sessionId: v.id("pitchSessions") },
  handler: async (ctx, args): Promise<any> => {
    console.log("üß† Starting report card generation for session:", args.sessionId);

    // Get session data
    const session: any = await ctx.runQuery(internal.sessions.getSessionInternal, {
      sessionId: args.sessionId,
    });

    if (!session) {
      console.error("‚ùå Session not found:", args.sessionId);
      throw new Error("Session not found");
    }

    console.log("üìù Session data retrieved:", {
      hasTranscript: !!session.transcript,
      transcriptLength: session.transcript?.length || 0,
      hasPitchContext: !!session.pitchContextText,
    });

    // Get all interruptions for context
    const interruptions = await ctx.runQuery(
      internal.sessions.getInterruptionsInternal,
      { sessionId: args.sessionId }
    );

    console.log("üìã Interruptions retrieved:", interruptions.length);

    // Build context for Gemini
    const prompt = `You are a hardcore VC analyst using the Bill Payne Scorecard method. Analyze this pitch session and generate a comprehensive Fundability Report Card.

PITCH CONTEXT:
${session.pitchContextText || "No written context provided"}

TRANSCRIPT:
${session.transcript || "Session ended without transcript"}

INTERRUPTIONS & FOUNDER RESPONSES:
${interruptions.map((i: { triggerType: string; founderStatement: string; vcResponse: string; founderReaction?: string }) => `[${i.triggerType}] Founder: "${i.founderStatement}" | VC: "${i.vcResponse}" | Reaction: ${i.founderReaction || "unknown"}`).join("\n")}

Evaluate on these 4 pillars (0-100 each):
1. MARKET CLARITY: How well-defined is the market? Are TAM/SAM claims credible? Are competitors acknowledged?
2. TECH DEFENSIBILITY: Is there real IP or moat? Or is this just a "GPT wrapper"?
3. UNIT ECONOMIC LOGIC: Do the CAC, LTV, and runway numbers make sense? Is pricing defensible?
4. INVESTOR READINESS: How coachable is the founder? Did they get defensive during interruptions?

Calculate a COACHABILITY DELTA based on founder reactions:
- Defensive reactions: SUBTRACT 10 points from Investor Readiness
- Receptive reactions: ADD 5 points
- Neutral: No change

Provide ONLY a JSON response with this EXACT structure (no markdown, no code blocks, just raw JSON):
{
  "marketClarity": <0-100>,
  "techDefensibility": <0-100>,
  "unitEconomicLogic": <0-100>,
  "investorReadiness": <0-100>,
  "overallScore": <average of 4 pillars>,
  "coachabilityDelta": <calculated from reactions>,
  "insights": "<brutally honest 3-5 sentence assessment>"
}`;

    try {
      console.log("ü§ñ Initializing Gemini API...");
      const genAI = getGeminiClient();
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

      console.log("üì§ Sending request to Gemini...");
      const result = await model.generateContent(prompt);
      const response = result.response;
      const text = response.text();

      console.log("üì• Received response from Gemini, length:", text.length);
      console.log("üìÑ Raw response preview:", text.substring(0, 200));

      // Clean up the response (remove markdown code blocks if present)
      const cleanText = text.replace(/```json\n?|\n?```/g, "").trim();

      console.log("üßπ Cleaned response, parsing JSON...");

      // Parse the JSON response
      const reportCard = JSON.parse(cleanText);

      console.log("‚úÖ Report card parsed successfully:", {
        overallScore: reportCard.overallScore,
        hasInsights: !!reportCard.insights,
      });

      // Save to database
      console.log("üíæ Saving report card to database...");
      await ctx.runMutation(internal.sessions.updateReportCard, {
        sessionId: args.sessionId,
        reportCard,
      });

      console.log("üéâ Report card generation complete!");
      return reportCard;
    } catch (error: any) {
      console.error("‚ùå Failed to generate report card:", error);
      console.error("‚ùå Error details:", {
        message: error.message,
        stack: error.stack,
        name: error.name,
      });

      // Create a fallback report card
      console.log("üîÑ Creating fallback report card...");
      const fallbackReportCard: any = {
        marketClarity: 50,
        techDefensibility: 50,
        unitEconomicLogic: 50,
        investorReadiness: 50,
        overallScore: 50,
        coachabilityDelta: 0,
        insights: `Unable to generate full analysis due to technical error. Based on available data: ${
          session.transcript
            ? "Transcript captured successfully"
            : "No transcript available"
        }. ${
          session.pitchContextText
            ? "Pitch context provided"
            : "No pitch context provided"
        }. Please review your session data and try again.`,
      };

      // Save fallback to database so user isn't stuck
      await ctx.runMutation(internal.sessions.updateReportCard, {
        sessionId: args.sessionId,
        reportCard: fallbackReportCard,
      });

      console.log("‚ö†Ô∏è Fallback report card saved");
      return fallbackReportCard;
    }
  },
});

// Process audio chunk for interruption detection
export const processAudioChunk = action({
  args: {
    sessionId: v.id("pitchSessions"),
    audioData: v.string(), // Base64 encoded audio
    transcript: v.string(),
  },
  handler: async (ctx, args) => {
    console.log("üîç Processing audio chunk for session:", args.sessionId);
    console.log("üìù Current transcript length:", args.transcript.length);

    // Get existing interruptions to enforce frequency limits
    const interruptions: any = await ctx.runQuery(
      internal.sessions.getInterruptionsInternal,
      { sessionId: args.sessionId }
    );

    console.log("üìã Existing interruptions:", interruptions.length);

    // FREQUENCY LIMIT: Maximum 3 interruptions total (LOW frequency as requested)
    if (interruptions.length >= 3) {
      console.log("‚ö†Ô∏è Max interruptions reached (3/3)");
      return { interrupted: false };
    }

    // MINIMUM TRANSCRIPT LENGTH: Don't interrupt too early
    if (args.transcript.length < 100) {
      console.log("‚è≥ Waiting for more content...");
      return { interrupted: false };
    }

    // Analyze transcript for trigger patterns
    const text = args.transcript.toLowerCase();

    let triggerType: "reality_check" | "math_check" | "bs_detector" | null =
      null;
    let vcResponse = "";

    // Reality Check triggers
    if (
      text.includes("no competitors") ||
      text.includes("no competition") ||
      text.includes("first to market") ||
      text.includes("unique in the world")
    ) {
      triggerType = "reality_check";

      // Use Tavily for tactical fact-checking
      const factCheck: any = await ctx.runAction(internal.tavily.tacticalFactCheck, {
        sessionId: args.sessionId,
        founderClaim: args.transcript,
        triggerType: "reality_check",
      });

      vcResponse = await generateVCInterruption(
        "reality_check",
        args.transcript,
        factCheck.success ? factCheck.facts : undefined
      );
    }

    // Math Check triggers
    else if (
      text.includes("hiring") ||
      text.includes("marketing spend") ||
      text.includes("runway") ||
      text.includes("burn rate")
    ) {
      triggerType = "math_check";

      // Use Tavily for industry benchmarks
      const factCheck: any = await ctx.runAction(internal.tavily.tacticalFactCheck, {
        sessionId: args.sessionId,
        founderClaim: args.transcript,
        triggerType: "math_check",
      });

      vcResponse = await generateVCInterruption(
        "math_check",
        args.transcript,
        factCheck.success ? factCheck.facts : undefined
      );
    }

    // BS Detector triggers
    else if (
      text.includes("proprietary ai") ||
      text.includes("our ai") ||
      text.includes("machine learning") ||
      text.includes("blockchain")
    ) {
      triggerType = "bs_detector";

      // Use Tavily for technology validation
      const factCheck: any = await ctx.runAction(internal.tavily.tacticalFactCheck, {
        sessionId: args.sessionId,
        founderClaim: args.transcript,
        triggerType: "bs_detector",
      });

      vcResponse = await generateVCInterruption(
        "bs_detector",
        args.transcript,
        factCheck.success ? factCheck.facts : undefined
      );
    }

    // If triggered, add interruption (AGGRESSIVE - no pause required)
    if (triggerType && vcResponse) {
      console.log("üö® TRIGGER DETECTED:", triggerType);
      console.log("üí¨ VC Response:", vcResponse);
      console.log("‚ö° INTERRUPTING MID-SENTENCE (aggressive mode)");

      await ctx.runMutation(internal.sessions.addInterruptionInternal, {
        sessionId: args.sessionId,
        triggerType,
        founderStatement: args.transcript.substring(args.transcript.length - 200), // Last 200 chars
        vcResponse,
      });

      console.log("‚úÖ Interruption saved to database");
      return { interrupted: true, vcResponse };
    }

    return { interrupted: false };
  },
});

// Generate VC interruption using Groq + Llama 3.1 70B (ultra-fast for real-time)
async function generateVCInterruption(
  triggerType: string,
  founderStatement: string,
  tavilyFacts?: Array<{ source: string; fact: string; url: string }>
): Promise<string> {
  // Build context with Tavily facts if available
  const factsContext = tavilyFacts && tavilyFacts.length > 0
    ? `\n\nREAL MARKET DATA (from Tavily search):\n${tavilyFacts.map((f) => `- ${f.source}: ${f.fact}`).join("\n")}`
    : "";

  const prompts = {
    reality_check: `The founder just claimed: "${founderStatement}"${factsContext}

You are a hardcore VC. Challenge this claim about market/competitors. ${tavilyFacts ? "Use the real market data above to cite specific competitors or facts." : "Be sharp and direct."} Keep it to 1-2 sentences.`,

    math_check: `The founder mentioned: "${founderStatement}"${factsContext}

You are a hardcore VC. Ask about the underlying numbers: CAC, LTV, runway, burn rate. ${tavilyFacts ? "Reference industry benchmarks from the data above if relevant." : "Be direct and expect precision."} Keep it to 1-2 sentences.`,

    bs_detector: `The founder said: "${founderStatement}"${factsContext}

You are a hardcore VC who hates buzzwords. Call out if this sounds like a "GPT wrapper" or generic tech claim. ${tavilyFacts ? "Use the real data above to validate or challenge the technology claim." : "Demand specifics about the actual IP or moat."} Keep it to 1-2 sentences.`,
  };

  try {
    const groq = getGroqClient();

    const chatCompletion = await groq.chat.completions.create({
      messages: [
        {
          role: "system",
          content: "You are a hardcore Silicon Valley VC conducting a brutal pitch interrogation. Be sharp, direct, and unforgiving. When given market data, cite it specifically.",
        },
        {
          role: "user",
          content: prompts[triggerType as keyof typeof prompts],
        },
      ],
      model: "llama-3.1-70b-versatile", // Fast and powerful
      temperature: 0.8,
      max_tokens: 150,
    });

    return chatCompletion.choices[0]?.message?.content || "Explain that claim.";
  } catch (error) {
    console.error("Groq API error:", error);
    // Fallback to a generic response if Groq fails
    return "That's an interesting claim. Can you provide more specifics?";
  }
}

// Evaluate founder's response to VC interruption
export const evaluateFounderResponse = action({
  args: {
    interruptionId: v.id("interruptions"),
    sessionId: v.id("pitchSessions"),
    founderResponse: v.string(),
    vcQuestion: v.string(),
  },
  handler: async (ctx, args) => {
    console.log("üîç Evaluating founder response...");

    try {
      const groq = getGroqClient();

      const evaluation = await groq.chat.completions.create({
        messages: [
          {
            role: "system",
            content: "You are evaluating a founder's response to a tough VC question. Analyze their tone, defensiveness, and quality of response. Reply with ONLY one word: defensive, receptive, or neutral."
          },
          {
            role: "user",
            content: `VC asked: "${args.vcQuestion}"\n\nFounder responded: "${args.founderResponse}"\n\nHow was the founder's reaction?`
          }
        ],
        model: "llama-3.1-70b-versatile",
        temperature: 0.3,
        max_tokens: 10,
      });

      const reaction = evaluation.choices[0]?.message?.content?.toLowerCase().trim() as "defensive" | "receptive" | "neutral" | undefined;

      console.log("üìä Founder reaction:", reaction);

      // Update interruption with reaction via internal mutation
      if (reaction && (reaction === "defensive" || reaction === "receptive" || reaction === "neutral")) {
        await ctx.runMutation(internal.sessions.updateInterruptionReaction, {
          interruptionId: args.interruptionId,
          founderReaction: reaction,
        });
      }

      return { reaction: reaction || "neutral" };
    } catch (error) {
      console.error("‚ùå Failed to evaluate response:", error);
      return { reaction: "neutral" };
    }
  },
});

// Socratic mentorship chat using Google Gemini 1.5 Flash
export const socraticChat = action({
  args: {
    sessionId: v.id("pitchSessions"),
    focusArea: v.union(
      v.literal("market"),
      v.literal("tech"),
      v.literal("economics"),
      v.literal("readiness")
    ),
    userMessage: v.string(),
    conversationHistory: v.array(
      v.object({
        role: v.union(v.literal("user"), v.literal("assistant")),
        content: v.string(),
      })
    ),
  },
  handler: async (ctx, args): Promise<string> => {
    const session: any = await ctx.runQuery(internal.sessions.getSessionInternal, {
      sessionId: args.sessionId,
    });

    if (!session || !session.reportCard) {
      throw new Error("Session or report card not found");
    }

    const systemContext = `You are a Socratic mentor helping a founder improve their business model.

CONTEXT FROM THEIR PITCH SESSION:
- Market Clarity Score: ${session.reportCard.marketClarity}/100
- Tech Defensibility Score: ${session.reportCard.techDefensibility}/100
- Unit Economic Logic Score: ${session.reportCard.unitEconomicLogic}/100
- Investor Readiness Score: ${session.reportCard.investorReadiness}/100
- Coachability Delta: ${session.reportCard.coachabilityDelta}

FOCUS AREA: ${args.focusArea}

Your role is to:
1. Ask probing questions (Socratic method)
2. Challenge assumptions constructively
3. Guide them to discover better solutions themselves
4. Reference specific issues from their pitch session
5. Be supportive but intellectually rigorous

Do not give direct answers. Ask questions that make them think deeper.`;

    // Build conversation history for Gemini
    const conversationText = args.conversationHistory
      .map((msg) => `${msg.role === "user" ? "Founder" : "Mentor"}: ${msg.content}`)
      .join("\n\n");

    const fullPrompt = `${systemContext}

CONVERSATION SO FAR:
${conversationText}

Founder: ${args.userMessage}

Mentor (respond with Socratic questions):`;

    try {
      const genAI = getGeminiClient();
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

      const result = await model.generateContent(fullPrompt);
      const response = result.response;
      const assistantMessage = response.text().trim();

      // Save to chat history
      await ctx.runMutation(internal.sessions.addChatMessage, {
        sessionId: args.sessionId,
        focusArea: args.focusArea,
        userMessage: args.userMessage,
        assistantMessage,
      });

      return assistantMessage;
    } catch (error) {
      console.error("Gemini API error:", error);
      throw error;
    }
  },
});
